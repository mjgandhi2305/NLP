{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b5d7919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from collections import Counter\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04a39765",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ai4bharat/IndicCorpV2\", \"indiccorp_v2\", split=\"guj_Gujr\", streaming=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44442205",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_end = re.compile(r'([\\.!?])\\s+')\n",
    "def tokenize_sentences(paragraph: str) -> list:\n",
    "    # Normalize whitespace\n",
    "    text = paragraph.strip().replace(\"\\n\", \" \")\n",
    "    \n",
    "    # Split on '.', '!' or '?' plus following space\n",
    "    parts = sentence_end.split(text)\n",
    "    \n",
    "    sentences = []\n",
    "    for i in range(0, len(parts) - 1, 2):\n",
    "        sent = parts[i] + parts[i+1]\n",
    "        sentences.append(sent.strip())\n",
    "    \n",
    "    if len(parts) % 2 == 1 and parts[-1].strip():\n",
    "        sentences.append(parts[-1].strip())\n",
    "    \n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd171c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenization\n",
    "# Unicode-aware regex patterns\n",
    "URL_PATTERN    = r'https?://[^\\s]+\\.\\w{2,}'\n",
    "EMAIL_PATTERN  = r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+'\n",
    "DATE_PATTERN   = r'\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b'\n",
    "DECIMAL_PATTERN = r'\\d+\\.\\d+'\n",
    "NUMBER_PATTERN = r'\\d+'\n",
    "# Gujarati words range: \\u0A80â€“\\u0AFF;\n",
    "GUJ_WORD       = r'[\\u0A80-\\u0AFF]+'\n",
    "PUNCT_PATTERN  = r'[^\\w\\s\\u0A80-\\u0AFF]'  # punctuation not part of Gujarati\n",
    "\n",
    "# Combine all token rules\n",
    "token_re = re.compile(\n",
    "    f\"{URL_PATTERN}|{EMAIL_PATTERN}|{DATE_PATTERN}|{DECIMAL_PATTERN}|{NUMBER_PATTERN}|{GUJ_WORD}|{PUNCT_PATTERN}\"\n",
    ")\n",
    "\n",
    "def tokenize_words(sentence):\n",
    "    return token_re.findall(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ffb45e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentences = 100000  # Increased for better corpus size\n",
    "batch_size = 10000     # Process in batches for memory efficiency\n",
    "output_dir = \"tokenized_gujarati_corpus\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cab30850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_batch(sentences_batch, batch_num):\n",
    "    \"\"\"Process a batch of sentences and save to parquet\"\"\"\n",
    "    tokenized_sentences = []\n",
    "    \n",
    "    for sent in sentences_batch:\n",
    "        tokens = tokenize_words(sent)\n",
    "        if tokens:  # Only include sentences with tokens\n",
    "            # Join tokens with spaces to create tokenized sentence\n",
    "            tokenized_sentence = ' '.join(tokens)\n",
    "            tokenized_sentences.append({\n",
    "                'original_sentence': sent,\n",
    "                'tokenized_sentence': tokenized_sentence,\n",
    "                'token_count': len(tokens)\n",
    "            })\n",
    "    \n",
    "    if tokenized_sentences:\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(tokenized_sentences)\n",
    "        \n",
    "        # Save as parquet with compression\n",
    "        output_file = os.path.join(output_dir, f\"gujarati_tokenized_batch_{batch_num:04d}.parquet\")\n",
    "        df.to_parquet(\n",
    "            output_file, \n",
    "            compression='snappy',  # Fast compression with good ratio\n",
    "            engine='pyarrow'\n",
    "        )\n",
    "        \n",
    "        print(f\"Saved batch {batch_num} with {len(tokenized_sentences)} sentences to {output_file}\")\n",
    "        return len(tokenized_sentences), df['tokenized_sentence'].tolist()\n",
    "    \n",
    "    return 0, []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c914249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting corpus processing...\n"
     ]
    }
   ],
   "source": [
    "# Process dataset in batches\n",
    "sentence_count = 0\n",
    "batch_num = 0\n",
    "current_batch = []\n",
    "all_tokens = []\n",
    "batch_stats = []\n",
    "\n",
    "print(\"Starting corpus processing...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b8363d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 0 with 9999 sentences to tokenized_gujarati_corpus\\gujarati_tokenized_batch_0000.parquet\n",
      "Saved batch 1 with 9998 sentences to tokenized_gujarati_corpus\\gujarati_tokenized_batch_0001.parquet\n",
      "Saved batch 2 with 9998 sentences to tokenized_gujarati_corpus\\gujarati_tokenized_batch_0002.parquet\n",
      "Saved batch 3 with 9998 sentences to tokenized_gujarati_corpus\\gujarati_tokenized_batch_0003.parquet\n",
      "Saved batch 4 with 10000 sentences to tokenized_gujarati_corpus\\gujarati_tokenized_batch_0004.parquet\n",
      "Saved batch 5 with 9998 sentences to tokenized_gujarati_corpus\\gujarati_tokenized_batch_0005.parquet\n",
      "Saved batch 6 with 9998 sentences to tokenized_gujarati_corpus\\gujarati_tokenized_batch_0006.parquet\n",
      "Saved batch 7 with 9999 sentences to tokenized_gujarati_corpus\\gujarati_tokenized_batch_0007.parquet\n",
      "Saved batch 8 with 9999 sentences to tokenized_gujarati_corpus\\gujarati_tokenized_batch_0008.parquet\n",
      "Saved batch 9 with 9999 sentences to tokenized_gujarati_corpus\\gujarati_tokenized_batch_0009.parquet\n"
     ]
    }
   ],
   "source": [
    "for data in dataset:\n",
    "    if max_sentences is not None and sentence_count >= max_sentences:\n",
    "        break\n",
    "    \n",
    "    text = data.get(\"text\", \"\").strip()\n",
    "    if not text:\n",
    "        continue\n",
    "    \n",
    "    sentences = tokenize_sentences(text)\n",
    "    for sent in sentences:\n",
    "        if max_sentences is not None and sentence_count >= max_sentences:\n",
    "            break\n",
    "        \n",
    "        current_batch.append(sent)\n",
    "        sentence_count += 1\n",
    "        \n",
    "        # Process batch when it reaches batch_size\n",
    "        if len(current_batch) >= batch_size:\n",
    "            batch_sentences, batch_tokenized = process_and_save_batch(current_batch, batch_num)\n",
    "            if batch_sentences > 0:\n",
    "                # Collect tokens for overall statistics\n",
    "                for tokenized_sent in batch_tokenized:\n",
    "                    all_tokens.extend(tokenized_sent.split())\n",
    "                \n",
    "                batch_stats.append({\n",
    "                    'batch_num': batch_num,\n",
    "                    'sentences': batch_sentences\n",
    "                })\n",
    "            \n",
    "            current_batch = []\n",
    "            batch_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3959fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process remaining sentences in the last batch\n",
    "if current_batch:\n",
    "    batch_sentences, batch_tokenized = process_and_save_batch(current_batch, batch_num)\n",
    "    if batch_sentences > 0:\n",
    "        for tokenized_sent in batch_tokenized:\n",
    "            all_tokens.extend(tokenized_sent.split())\n",
    "        \n",
    "        batch_stats.append({\n",
    "            'batch_num': batch_num,\n",
    "            'sentences': batch_sentences\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb83b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sentences = sum(stat['sentences'] for stat in batch_stats)\n",
    "total_words = len(all_tokens)\n",
    "total_characters = sum(len(token) for token in all_tokens)\n",
    "unique_tokens = set(all_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c29b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sentence_length = total_words / total_sentences if total_sentences else 0\n",
    "avg_word_length = total_characters / total_words if total_words else 0\n",
    "type_token_ratio = len(unique_tokens) / total_words if total_words else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "716b28b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ðŸ“Š FINAL CORPUS STATISTICS\n",
      "==================================================\n",
      "i.   Total number of sentences         : 99,986\n",
      "ii.  Total number of words             : 1,678,173\n",
      "iii. Total number of characters        : 7,360,779\n",
      "iv.  Average sentence length (words)   : 16.78\n",
      "v.   Average word length (characters)  : 4.39\n",
      "vi.  Type/Token Ratio (TTR)            : 0.0802\n",
      "vii. Number of batches created         : 10\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ðŸ“Š FINAL CORPUS STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"i.   Total number of sentences         : {total_sentences:,}\")\n",
    "print(f\"ii.  Total number of words             : {total_words:,}\")\n",
    "print(f\"iii. Total number of characters        : {total_characters:,}\")\n",
    "print(f\"iv.  Average sentence length (words)   : {avg_sentence_length:.2f}\")\n",
    "print(f\"v.   Average word length (characters)  : {avg_word_length:.2f}\")\n",
    "print(f\"vi.  Type/Token Ratio (TTR)            : {type_token_ratio:.4f}\")\n",
    "print(f\"vii. Number of batches created         : {len(batch_stats)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d280690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = Counter(all_tokens)\n",
    "most_common = token_counts.most_common(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be89122c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¢ Top 100 Most Frequent Words:\n",
      " 1. .                â€”  96,827 times\n",
      " 2. àª›à«‡               â€”  63,146 times\n",
      " 3. ,                â€”  46,822 times\n",
      " 4. àª…àª¨à«‡              â€”  28,565 times\n",
      " 5. àª†                â€”  18,318 times\n",
      " 6. àª•à«‡               â€”  16,257 times\n",
      " 7. àªªàª£               â€”  13,070 times\n",
      " 8. àª®àª¾àªŸà«‡             â€”  12,940 times\n",
      " 9. -                â€”  10,646 times\n",
      "10. àªàª•               â€”  9,998 times\n",
      "11. àª•àª°à«€              â€”  9,814 times\n",
      "12. àªªàª°               â€”  9,669 times\n",
      "13. àª¤à«‡               â€”  8,503 times\n",
      "14. àªœ                â€”  8,414 times\n",
      "15. àª¸àª¾àª¥à«‡             â€”  8,322 times\n",
      "16. àª¹àª¤à«€              â€”  8,270 times\n",
      "17. àª¤à«‹               â€”  6,216 times\n",
      "18. àª¹àª¤à«‹              â€”  5,776 times\n",
      "19. àª¹àª¤àª¾              â€”  5,490 times\n",
      "20. àª¨àª¥à«€              â€”  5,162 times\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸ”¢ Top 100 Most Frequent Words:\")\n",
    "for i, (word, freq) in enumerate(most_common, 1):\n",
    "    print(f\"{i:>2}. {word:<15}  â€”  {freq:,} times\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1691891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 96827),\n",
       " ('àª›à«‡', 63146),\n",
       " (',', 46822),\n",
       " ('àª…àª¨à«‡', 28565),\n",
       " ('àª†', 18318),\n",
       " ('àª•à«‡', 16257),\n",
       " ('àªªàª£', 13070),\n",
       " ('àª®àª¾àªŸà«‡', 12940),\n",
       " ('-', 10646),\n",
       " ('àªàª•', 9998),\n",
       " ('àª•àª°à«€', 9814),\n",
       " ('àªªàª°', 9669),\n",
       " ('àª¤à«‡', 8503),\n",
       " ('àªœ', 8414),\n",
       " ('àª¸àª¾àª¥à«‡', 8322),\n",
       " ('àª¹àª¤à«€', 8270),\n",
       " ('àª¤à«‹', 6216),\n",
       " ('àª¹àª¤à«‹', 5776),\n",
       " ('àª¹àª¤àª¾', 5490),\n",
       " ('àª¨àª¥à«€', 5162)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d68b9aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_unique_tokens = len(token_counts)\n",
    "hapax_legomena = sum(1 for count in token_counts.values() if count == 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc83724c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Frequency Distribution:\n",
      "Total unique tokens      : 134,613\n",
      "Hapax legomena (freq=1)  : 76,792\n",
      "Hapax percentage         : 57.05%\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nðŸ“ˆ Frequency Distribution:\")\n",
    "print(f\"Total unique tokens      : {total_unique_tokens:,}\")\n",
    "print(f\"Hapax legomena (freq=1)  : {hapax_legomena:,}\")\n",
    "print(f\"Hapax percentage         : {(hapax_legomena/total_unique_tokens)*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "233d4d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Files saved in directory: tokenized_gujarati_corpus/\n",
      "   - gujarati_tokenized_batch_XXXX.parquet (tokenized sentences)\n",
      "   - corpus_statistics.parquet (overall statistics)\n",
      "   - word_frequencies_top20.parquet (most frequent words)\n"
     ]
    }
   ],
   "source": [
    "# Save overall statistics to a separate file\n",
    "stats_data = {\n",
    "    'metric': [\n",
    "        'Total Sentences', 'Total Words', 'Total Characters', \n",
    "        'Average Sentence Length', 'Average Word Length', 'Type-Token Ratio',\n",
    "        'Unique Tokens', 'Hapax Legomena'\n",
    "    ],\n",
    "    'value': [\n",
    "        total_sentences, total_words, total_characters,\n",
    "        avg_sentence_length, avg_word_length, type_token_ratio,\n",
    "        total_unique_tokens, hapax_legomena\n",
    "    ]\n",
    "}\n",
    "\n",
    "stats_df = pd.DataFrame(stats_data)\n",
    "stats_file = os.path.join(output_dir, \"corpus_statistics.parquet\")\n",
    "stats_df.to_parquet(stats_file, compression='snappy')\n",
    "\n",
    "# Save top words frequency\n",
    "freq_data = pd.DataFrame(most_common, columns=['word', 'frequency'])\n",
    "freq_file = os.path.join(output_dir, \"word_frequencies_top20.parquet\")\n",
    "freq_data.to_parquet(freq_file, compression='snappy')\n",
    "\n",
    "print(f\"\\nðŸ’¾ Files saved in directory: {output_dir}/\")\n",
    "print(\"   - gujarati_tokenized_batch_XXXX.parquet (tokenized sentences)\")\n",
    "print(\"   - corpus_statistics.parquet (overall statistics)\")\n",
    "print(\"   - word_frequencies_top20.parquet (most frequent words)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee4f76a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“– Example: Reading saved data\n",
      "------------------------------\n",
      "Sample from first batch (showing first 3 rows):\n",
      "Original: àª† àªµà«€àª¡àª¿àª¯à«‹ àªœà«àª“: àªŠàª‚àªàª¾ àª®àª¾àª°à«àª•à«‡àªŸàª¯àª¾àª°à«àª¡ àª†àªœàª¥à«€ 25 àªœà«àª²àª¾àªˆ àª¸à«àª§à«€...\n",
      "Tokenized: àª† àªµà«€àª¡àª¿àª¯à«‹ àªœà«àª“ : àªŠàª‚àªàª¾ àª®àª¾àª°à«àª•à«‡àªŸàª¯àª¾àª°à«àª¡ àª†àªœàª¥à«€ 25 àªœà«àª²àª¾àªˆ àª¸à«àª§...\n",
      "Tokens: 11\n",
      "\n",
      "Original: àª®àª¿àª¥à«‡àª¨à«‹àª² àª†àªµà«àª¯à«‹ àª•à«àª¯àª¾àª‚àª¥à«€?...\n",
      "Tokenized: àª®àª¿àª¥à«‡àª¨à«‹àª² àª†àªµà«àª¯à«‹ àª•à«àª¯àª¾àª‚àª¥à«€ ?...\n",
      "Tokens: 4\n",
      "\n",
      "Original: àª†àª–àª°à«‡ àª¤à«àª°àª£ àª°àª¾àªœà«àª¯à«‹àª®àª¾àª‚ àª®àª³à«‡àª² àª¹àª¾àª° àªªàª° àª•à«‹àª‚àª—à«àª°à«‡àª¸ àª…àª§à«àª¯àª•à«àª· àª°...\n",
      "Tokenized: àª†àª–àª°à«‡ àª¤à«àª°àª£ àª°àª¾àªœà«àª¯à«‹àª®àª¾àª‚ àª®àª³à«‡àª² àª¹àª¾àª° àªªàª° àª•à«‹àª‚àª—à«àª°à«‡àª¸ àª…àª§à«àª¯àª•à«àª· àª°...\n",
      "Tokens: 17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸ“– Example: Reading saved data\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Read first batch\n",
    "first_batch_file = os.path.join(output_dir, \"gujarati_tokenized_batch_0000.parquet\")\n",
    "if os.path.exists(first_batch_file):\n",
    "    sample_df = pd.read_parquet(first_batch_file)\n",
    "    print(f\"Sample from first batch (showing first 3 rows):\")\n",
    "    for idx in range(min(3, len(sample_df))):\n",
    "        print(f\"Original: {sample_df.iloc[idx]['original_sentence'][:50]}...\")\n",
    "        print(f\"Tokenized: {sample_df.iloc[idx]['tokenized_sentence'][:50]}...\")\n",
    "        print(f\"Tokens: {sample_df.iloc[idx]['token_count']}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bac26fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_tokenized_sentences(directory):\n",
    "    \"\"\"Load all tokenized sentences from parquet files\"\"\"\n",
    "    all_sentences = []\n",
    "    \n",
    "    for filename in sorted(os.listdir(directory)):\n",
    "        if filename.startswith(\"gujarati_tokenized_batch_\") and filename.endswith(\".parquet\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            df = pd.read_parquet(filepath)\n",
    "            all_sentences.extend(df['tokenized_sentence'].tolist())\n",
    "    \n",
    "    return all_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e948f2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loaded sentences: 99986\n"
     ]
    }
   ],
   "source": [
    "all_tokenized = load_all_tokenized_sentences(output_dir)\n",
    "print(f\"Total loaded sentences: {len(all_tokenized)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4efc2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
